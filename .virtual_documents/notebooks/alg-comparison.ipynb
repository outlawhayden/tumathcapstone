# imports
import numpy as np
from sklearn.metrics import mean_squared_error
import seaborn as sns
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingGridSearchCV








# x between -1 and 1
# y = x^2 + |x| + σ, where σ is N(0, 0.01)
# training data, 20 data points from interval, 400 points as testing data
# rerun algorithms 10 times with different seeds, track mean values and CIs of predicted results

def generate_data(x_lower, x_upper, mean, std, n, seed = 0):
    np.random.seed(seed)
    x_vals = np.random.uniform(x_lower, x_upper, n)
    error_vals = np.random.normal(mean, std, n)
    data =  [(x, (x**2 + abs(x) + n)) for (x, n) in zip(x_vals, error_vals)]
    #return(data)
    x = np.array([d[0] for d in data]).reshape(-1,1)
    y = np.array([d[1] for d in data])
    return x,y

X_train, y_train = generate_data(-1,1,0,0.01,40,0)
X_test, y_test = generate_data(-1,1,0,0.01,400,1)



plt.scatter(X_train, y_train)


plt.scatter(X_test, y_test)





# Random Forest

from sklearn.ensemble import RandomForestRegressor

params = {'n_estimators': (10, 100, 1000), 'min_weight_fraction_leaf': (0.0, 0.25, 0.5), 'max_features': ('sqrt', 'log2', None)}

rfr = RandomForestRegressor(n_estimators = 10, random_state = 0)
#n_estimators was 10 when paper was written

rfr_search = HalvingGridSearchCV(rfr, params, resource = 'n_samples', random_state = 0, n_jobs = -1).fit(X_train, y_train)

rfr.fit(X_train, y_train)

y_pred = rfr_search.predict(X_test)
print(mean_squared_error(y_test, y_pred))


# Rotation Forest
# No Implementation?


# Extremely Randomized Trees
from sklearn.ensemble import ExtraTreesRegressor

etr = ExtraTreesRegressor(n_estimators = 10, random_state = 0)

etr.fit(X_train, y_train)
y_pred = etr.predict(X_test)
print(mean_squared_error(y_test, y_pred))


# AdaBoost
from sklearn.ensemble import AdaBoostRegressor

params =  {'learning_rate': (0.01, 0.1, 1.0, 10.0), 'n_estimators': (10, 100, 1000)}

abc = AdaBoostRegressor(random_state = 0)

abc_search = HalvingGridSearchCV(abc, params, resource = 'n_samples', random_state = 0, n_jobs = -1).fit(X_train, y_train)


y_pred = abc_search.predict(X_test)
print(mean_squared_error(y_test, y_pred))


# Gradient Boosted DT

from sklearn.ensemble import GradientBoostingRegressor

gbr = GradientBoostingRegressor(random_state = 0)
gbr.fit(X_train, y_train)

y_pred = gbr.predict(X_test)
print(mean_squared_error(y_test, y_pred))


%%time
# DART
import xgboost as xgb

params = {'n_estimators': (10, 50, 100, 250, 500, 1000), 'learning_rate': (0.0001, 0.01, 0.05, 0.1, 0.2), 'gamma': (0, 0.1, 0.2, 0.3, 0.4), 'subsample': (0.5, 0.75, 1)}

dart_xgb = xgb.XGBRegressor(eval_metric = mean_squared_error, booster = "dart", random_state = 0)

dart_xgb_search = HalvingGridSearchCV(dart_xgb, params, resource = 'n_samples', random_state = 0, n_jobs = -1).fit(X_train, y_train)


y_pred = dart_xgb_search.predict(X_test)
print(mean_squared_error(y_test, y_pred))


# XGBOOST

import xgboost as xgb

tree_xgb = xgb.XGBRegressor(eval_metric = mean_squared_error, random_state = 0)

tree_xgb.fit(X_train, y_train)

y_pred = tree_xgb.predict(X_test)
print(mean_squared_error(y_test, y_pred))





# CatBoost

from catboost import CatBoostRegressor

cbm = CatBoostRegressor(random_state = 0, silent = True)
cbm.fit(X_train, y_train)

y_pred = cbm.predict(X_test)
print(mean_squared_error(y_test, y_pred))


# Evolutionary Forest (From Paper)
from evolutionary_forest.forest import EvolutionaryForestRegressor
from evolutionary_forest.utils import get_feature_importance, plot_feature_importance, feature_append

# params from page 742 of original paper
efr = EvolutionaryForestRegressor(random_seed = 0, max_height = 8, n_pop = 60, n_gen = 100, cross_pb = 0.5, gene_num = 5)
#efr = EvolutionaryForestRegressor(random_seed = 0)
efr.fit(X_train, y_train)
y_pred = efr.predict(X_test)
print(mean_squared_error(y_test, y_pred))




